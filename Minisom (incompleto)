import numpy as np
from numpy import (array, unravel_index, nditer, linalg, random, subtract, max,
                   power, exp, zeros, ones, arange, outer, meshgrid, dot,
                   logical_and, mean, cov, argsort, linspace,
                   einsum, prod, nan, sqrt, hstack, diff, argmin, multiply,
                   nanmean, nansum, tile, array_equal, isclose)
from numpy.linalg import norm
from collections import defaultdict, Counter
from warnings import warn
from sys import stdout
from time import time
from datetime import timedelta
import pickle
import os
import xarray as xr
from skimage.metrics import structural_similarity as ssim
import matplotlib.pyplot as plt

# para testes de unidade
from numpy.testing import assert_almost_equal, assert_array_almost_equal
from numpy.testing import assert_array_equal
import unittest

"""
    Implementação minimalista costumizada dos Mapas Auto-Organizáveis 6x6 (SOM).
"""


def _build_iteration_indexes(data_len, num_iterations,
                             verbose=False, random_generator=None,
                             use_epochs=False):
    """Retorna um iterável com os índices das amostras
    para usar em cada iteração do treinamento."""
    if use_epochs:
        iterations_per_epoch = arange(data_len)
        if random_generator:
            random_generator.shuffle(iterations_per_epoch)
        iterations = tile(iterations_per_epoch, num_iterations)
    else:
        iterations = arange(num_iterations) % data_len
        if random_generator:
            random_generator.shuffle(iterations)
    if verbose:
        return _wrap_index__in_verbose(iterations)
    else:
        return iterations


def _wrap_index__in_verbose(iterations):
    """Gera os valores de iterações, mostrando o status no console."""
    m = len(iterations)
    digits = len(str(m))
    progress = '\r [ {s:{d}} / {m} ] {s:3.0f}% - ? it/s'
    progress = progress.format(m=m, d=digits, s=0)
    stdout.write(progress)
    beginning = time()
    stdout.write(progress)
    for i, it in enumerate(iterations):
        yield it
        sec_left = ((m-i+1) * (time() - beginning)) / (i+1)
        time_left = str(timedelta(seconds=sec_left))[:7]
        progress = '\r [ {i:{d}} / {m} ]'.format(i=i+1, d=digits, m=m)
        progress += ' {p:3.0f}%'.format(p=100*(i+1)/m)
        progress += ' - {time_left} left '.format(time_left=time_left)
        stdout.write(progress)


def fast_norm(x):
    """Retorna a norma-2 de um array 1-D numpy.
    """
    return sqrt(dot(x, x.T))


class MiniSom(object):
    Y_HEX_CONV_FACTOR = (3.0 / 2.0) / sqrt(3)

    def __init__(self, x, y, input_len, sigma=1, learning_rate=0.5,
                 decay_function='asymptotic_decay',
                 neighborhood_function='gaussian', topology='rectangular',
                 activation_distance='ssim', random_seed=None,
                 sigma_decay_function='asymptotic_decay'):
        """Inicializa um Mapa Auto-Organizável.
        
        Parâmetros
        ----------
        x : int
            dimensão x do SOM.

        y : int
            dimensão y do SOM.

        input_len : int
            Número de elementos dos vetores de entrada.
            
        sigma : float, opcional (padrão=1)
            Espalhamento da função de vizinhança.

        learning_rate : float, opcional (padrão=0.5)
            Taxa de aprendizado inicial.

        decay_function : string ou callable, opcional
            Função que reduz a taxa de aprendizado em cada iteração.
            Valores possíveis: 'inverse_decay_to_zero', 'linear_decay_to_zero', 'asymptotic_decay'

        neighborhood_function : string, opcional (padrão='gaussian')
            Função que pondera a vizinhança de uma posição no mapa.
            Valores possíveis: 'gaussian', 'mexican_hat', 'bubble', 'triangle'

        topology : string, opcional (padrão='rectangular')
            Topologia do mapa.
            Valores possíveis: 'rectangular', 'hexagonal'

        activation_distance : string, callable opcional (padrão='euclidean')
            Distância usada para ativar o mapa.
            Valores possíveis: 'euclidean', 'cosine', 'manhattan', 'chebyshev', 'ssim'

        random_seed : int, opcional (padrão=None)
            Semente aleatória a ser usada.

        sigma_decay_function : string, opcional
            Função que reduz sigma em cada iteração.
            Valores possíveis: 'inverse_decay_to_one', 'linear_decay_to_one', 'asymptotic_decay'
        """
        if sigma > sqrt(x*x + y*y):
            warn('Warning: sigma might be too high ' +
                 'for the dimension of the map.')

        self._random_generator = random.RandomState(random_seed)

        self._learning_rate = learning_rate
        self._sigma = sigma
        self._input_len = input_len
        # inicialização aleatória 
        self._weights = self._random_generator.rand(x, y, input_len)*2-1
        self._weights /= linalg.norm(self._weights, axis=-1, keepdims=True)

        self._activation_map = zeros((x, y))
        self._neigx = arange(x)
        self._neigy = arange(y)  # utilizado para avaliar as funções de vizinhança

        if topology not in ['hexagonal', 'rectangular']:
            msg = '%s not supported only hexagonal and rectangular available'
            raise ValueError(msg % topology)
        self.topology = topology
        self._xx, self._yy = meshgrid(self._neigx, self._neigy)
        self._xx = self._xx.astype(float)
        self._yy = self._yy.astype(float)
        if topology == 'hexagonal':
            self._xx[::-2] -= 0.5
            self._yy *= self.Y_HEX_CONV_FACTOR
            if neighborhood_function in ['triangle']:
                warn('triangle neighborhood function does not ' +
                     'take in account hexagonal topology')

        lr_decay_functions = {
            'inverse_decay_to_zero': self._inverse_decay_to_zero,
            'linear_decay_to_zero': self._linear_decay_to_zero,
            'asymptotic_decay': self._asymptotic_decay}

        if isinstance(decay_function, str):
            if decay_function not in lr_decay_functions:
                msg = '%s not supported. Functions available: %s'
                raise ValueError(msg % (decay_function,
                                        ', '.join(lr_decay_functions.keys())))

            self._learning_rate_decay_function = \
                lr_decay_functions[decay_function]
        elif callable(decay_function):
            self._learning_rate_decay_function = decay_function

        sig_decay_functions = {
            'inverse_decay_to_one': self._inverse_decay_to_one,
            'linear_decay_to_one': self._linear_decay_to_one,
            'asymptotic_decay': self._asymptotic_decay}

        if sigma_decay_function not in sig_decay_functions:
            msg = '%s not supported. Functions available: %s'
            raise ValueError(msg % (sigma_decay_function,
                                    ', '.join(sig_decay_functions.keys())))

        self._sigma_decay_function = sig_decay_functions[sigma_decay_function]

        neig_functions = {'gaussian': self._gaussian,
                          'mexican_hat': self._mexican_hat,
                          'bubble': self._bubble,
                          'triangle': self._triangle}

        if neighborhood_function not in neig_functions:
            msg = '%s not supported. Functions available: %s'
            raise ValueError(msg % (neighborhood_function,
                                    ', '.join(neig_functions.keys())))

        if neighborhood_function in ['triangle',
                                     'bubble'] and (divmod(sigma, 1)[1] != 0
                                                    or sigma < 1):
            warn('sigma should be an integer >=1 when triangle or bubble' +
                 'are used as neighborhood function')

        self.neighborhood = neig_functions[neighborhood_function]

        distance_functions = {'euclidean': self._euclidean_distance,
                              'cosine': self._cosine_distance,
                              'manhattan': self._manhattan_distance,
                              'chebyshev': self._chebyshev_distance,
                              'ssim': self._ssim}

        if isinstance(activation_distance, str):
            if activation_distance not in distance_functions:
                msg = '%s not supported. Distances available: %s'
                raise ValueError(msg % (activation_distance,
                                        ', '.join(distance_functions.keys())))

            self._activation_distance = distance_functions[activation_distance]
        elif callable(activation_distance):
            self._activation_distance = activation_distance

    def get_weights(self):
        """Retorna os pesos da rede neural."""
        return self._weights

    def get_euclidean_coordinates(self):
        """Retorna a posição dos neurônios em um plano euclidiano que reflete a topologia escolhida.
        """
        return self._xx.T, self._yy.T

    def convert_map_to_euclidean(self, xy):
        """Converte as coordenadas do mapa para coordenadas euclidianas.
        """
        return self._xx.T[xy], self._yy.T[xy]

    def _activate(self, x):
        """Atualiza a matriz de ativação."""
        self._activation_map = self._activation_distance(x, self._weights)

    def activate(self, x):
        """Retorna o mapa de ativação para x."""
        self._activate(x)
        return self._activation_map

    def _inverse_decay_to_zero(self, learning_rate, t, max_iter):
        """Função de decaimento da taxa de aprendizado que se aproxima assintoticamente de zero.
        """
        C = max_iter / 100.0
        return learning_rate * C / (C + t)

    def _linear_decay_to_zero(self, learning_rate, t, max_iter):
        """Função de decaimento da taxa de aprendizado que diminui linearmente para zero.
        """
        return learning_rate * (1 - t / max_iter)

    def _inverse_decay_to_one(self, sigma, t, max_iter):
        """Função de decaimento de sigma que se aproxima assintoticamente de um.
        """
        C = (sigma - 1) / max_iter
        return sigma / (1 + (t * C))

    def _linear_decay_to_one(self, sigma, t, max_iter):
        """Função de decaimento de sigma que diminui
        linearmente para um.
        """
        return sigma + (t * (1 - sigma) / max_iter)

    def _asymptotic_decay(self, dynamic_parameter, t, max_iter):
        """Função de decaimento que reduz o parâmetro
        dinâmico assintoticamente.
        """
        return dynamic_parameter / (1 + t / (max_iter / 2))

    def _gaussian(self, c, sigma):
        """Retorna uma Gaussiana centrada em c."""
        d = 2*sigma*sigma
        ax = exp(-power(self._xx-self._xx.T[c], 2)/d)
        ay = exp(-power(self._yy-self._yy.T[c], 2)/d)
        return (ax * ay).T  # the external product gives a matrix

    def _mexican_hat(self, c, sigma):
        """Mexican hat centrada em c."""
        p = power(self._xx-self._xx.T[c], 2) + power(self._yy-self._yy.T[c], 2)
        d = 2*sigma*sigma
        return (exp(-p/d)*(1-2/d*p)).T

    def _bubble(self, c, sigma):
        """Função constante centrada em c com espalhamento sigma."""
        ax = logical_and(self._neigx > c[0]-sigma,
                         self._neigx < c[0]+sigma)
        ay = logical_and(self._neigy > c[1]-sigma,
                         self._neigy < c[1]+sigma)
        return outer(ax, ay)*1.

    def _triangle(self, c, sigma):
        """Função triangular centrada em c com espalhamento sigma."""
        triangle_x = (-abs(c[0] - self._neigx)) + sigma
        triangle_y = (-abs(c[1] - self._neigy)) + sigma
        triangle_x[triangle_x < 0] = 0.
        triangle_y[triangle_y < 0] = 0.
        return outer(triangle_x, triangle_y)

    def _cosine_distance(self, x, w):
        num = (w * x).sum(axis=2)
        denum = multiply(linalg.norm(w, axis=2), linalg.norm(x))
        return 1 - num / (denum+1e-8)

    def _euclidean_distance(self, x, w):
        return linalg.norm(subtract(x, w), axis=-1)

    def _manhattan_distance(self, x, w):
        return linalg.norm(subtract(x, w), ord=1, axis=-1)

    def _ssim(self, x, w):

        def ssim_global(x, y):
            mean_x = np.mean(x)
            mean_y = np.mean(y)
            var_x = np.var(x)
            var_y = np.var(y)
            cov = np.mean((x - mean_x) * (y - mean_y))

            # print(mean_x,mean_y,var_x,var_y,cov)

            c1 = 0.01 ** 2
            c2 = 0.03 ** 2

            ssim = ((2 * mean_x * mean_y + c1) * (2 * cov + c2)) / (
                        (mean_x ** 2 + mean_y ** 2 + c1) * (var_x + var_y + c2))
            return ssim

        for i in range(w.shape[0]):
            for j in range(w.shape[1]):
                try:
                    self._activation_map[i, j] = ssim_global(x, w[i, j])
                    # print(f"Valor SSIM({i},{j}):",self._activation_map[i,j])
                except Exception as e:
                    # print(f"Erro em SSIM({i},{j}):", e)
                    self._activation_map[i, j] = -1
        return -self._activation_map
    
    def _chebyshev_distance(self, x, w):
        return max(subtract(x, w), axis=-1)

    def _check_iteration_number(self, num_iteration):
        if num_iteration < 1:
            raise ValueError('num_iteration must be > 1')

    def _check_input_len(self, data):
        """Verifica se os dados de entrada têm o formato correto."""
        data_len = len(data[0])
        if self._input_len != data_len:
            msg = 'Received %d features, expected %d.' % (data_len,
                                                          self._input_len)
            raise ValueError(msg)

    def winner(self, x):
        """Calcula as coordenadas do neurônio vencedor para a amostra x."""
        self._activate(x)
        return unravel_index(self._activation_map.argmin(),
                             self._activation_map.shape)

    def update(self, x, win, t, max_iteration):
        """Atualiza os pesos dos neurônios."""
        eta = self._learning_rate_decay_function(self._learning_rate,
                                                 t, max_iteration)
        sig = self._sigma_decay_function(self._sigma, t, max_iteration)
        # improves the performances
        g = self.neighborhood(win, sig)*eta
        # w_new = eta * neighborhood_function * (x-w)
        self._weights += einsum('ij, ijk->ijk', g, x-self._weights)

    def quantization(self, data):
        """Atribui um vetor de pesos do neurônio vencedor a cada amostra em data."""
        self._check_input_len(data)
        winners_coords = argmin(self._distance_from_weights(data), axis=1)
        return self._weights[unravel_index(winners_coords,
                                           self._weights.shape[:2])]

    def random_weights_init(self, data):
        """Inicializa os pesos do SOM pegando amostras aleatórias dos dados."""
        self._check_input_len(data)
        it = nditer(self._activation_map, flags=['multi_index'])
        while not it.finished:
            rand_i = self._random_generator.randint(len(data))
            self._weights[it.multi_index] = data[rand_i]
            it.iternext()

    def pca_weights_init(self, data):
        """Inicializa os pesos para cobrir os dois primeiros componentes principais."""
        if self._input_len == 1:
            msg = 'Os dados precisam de pelo menos 2 características para a inicialização PCA'
            raise ValueError(msg)
        self._check_input_len(data)
        if len(self._neigx) == 1 or len(self._neigy) == 1:
            msg = 'inicialização PCA inapropriada:' + \
                  'uma das dimensões do mapa é 1.'
            warn(msg)
        pc_length, eigvecs = linalg.eig(cov(data))
        pc = (eigvecs.T @ data)
        pc_order = argsort(-pc_length)
        for i, c1 in enumerate(linspace(-1, 1, len(self._neigx))):
            for j, c2 in enumerate(linspace(-1, 1, len(self._neigy))):
                self._weights[i, j] = c1*pc[pc_order[0]] + \
                                      c2*pc[pc_order[1]]

    def _check_fixed_points(self, fixed_points, data):
        for k in fixed_points.keys():
            if not isinstance(k, int):
                raise TypeError('os índices dos fixed_points devem ser inteiros.')
            if k >= len(data) or k < 0:
                raise ValueError('um índice de um fixed_point não pode ser maior que len(data) ou menor que 0.')
            if fixed_points[k][0] >= self._weights.shape[0] or \
               fixed_points[k][1] >= self._weights.shape[1]:
                raise ValueError('coordenadas para fixed_point estão fora dos limites.')
            if fixed_points[k][0] < 0 or \
               fixed_points[k][1] < 0:
                raise ValueError('as coordenadas não podem ser negativas.')

    def train(self, data, num_iteration,
              random_order=False, verbose=False,
              use_epochs=False, fixed_points=None):
        """Treina o SOM.

        Parâmetros
        ----------
        data : np.array ou lista
            Matriz de dados.

        num_iteration : int
            Número de iterações ou épocas.

        random_order : bool (padrão=False)
            Se True, as amostras são escolhidas em ordem aleatória.

        verbose : bool (padrão=False)
            Se True, o status do treinamento será impresso.

        use_epochs : bool (padrão=False)
            Se True, o treinamento será por número de épocas.

        fixed_points : dict (padrão=None)
            Um dicionário que força o algoritmo de treinamento a usar um neurônio específico como vencedor para uma amostra.
        """
        self._check_iteration_number(num_iteration)
        self._check_input_len(data)
        random_generator = None
        if random_order:
            random_generator = self._random_generator
        iterations = _build_iteration_indexes(len(data), num_iteration,
                                              verbose, random_generator,
                                              use_epochs)
        if use_epochs:
            def get_decay_rate(iteration_index, data_len):
                return int(iteration_index / data_len)
        else:
            def get_decay_rate(iteration_index, data_len):
                return int(iteration_index)

        if fixed_points:
            self._check_fixed_points(fixed_points, data)
        else:
            fixed_points = {}

        for t, iteration in enumerate(iterations):
            decay_rate = get_decay_rate(t, len(data))
            self.update(data[iteration],
                        fixed_points.get(iteration,
                                         self.winner(data[iteration])),
                        decay_rate, num_iteration)
        if verbose:
            print('\n erro de quantização:', self.quantization_error(data))

    def train_random(self, data, num_iteration, verbose=False):
        """Treina o SOM escolhendo amostras aleatoriamente."""
        self.train(data, num_iteration, random_order=True, verbose=verbose)

    def train_batch(self, data, num_iteration, verbose=False):
        """Treina o SOM usando todos os vetores sequencialmente."""
        self.train(data, num_iteration, random_order=False, verbose=verbose)

    def distance_map(self, scaling='sum'):
        """Retorna o mapa de distância dos pesos."""

        if scaling not in ['sum', 'mean']:
            raise ValueError(f'o "scaling" deve ser "sum" ou "mean" ("{scaling}" não é válido)')

        um = nan * zeros((self._weights.shape[0],
                          self._weights.shape[1],
                          8))  
        
        ii = [[0, -1, -1, -1, 0, 1, 1, 1]]*2
        jj = [[-1, -1, 0, 1, 1, 1, 0, -1]]*2

        if self.topology == 'hexagonal':
            ii = [[1, 1, 1, 0, -1, 0], [0, 1, 0, -1, -1, -1]]
            jj = [[1, 0, -1, -1, 0, 1], [1, 0, -1, -1, 0, 1]]

        for x in range(self._weights.shape[0]):
            for y in range(self._weights.shape[1]):
                w_2 = self._weights[x, y]
                e = y % 2 == 0   
                for k, (i, j) in enumerate(zip(ii[e], jj[e])):
                    if (x+i >= 0 and x+i < self._weights.shape[0] and
                            y+j >= 0 and y+j < self._weights.shape[1]):
                        w_1 = self._weights[x+i, y+j]
                        um[x, y, k] = fast_norm(w_2-w_1)

        if scaling == 'mean':
            um = nanmean(um, axis=2)
        if scaling == 'sum':
            um = nansum(um, axis=2)

        return um/um.max()

    def activation_response(self, data):
        """
            Retorna uma matriz onde o elemento i,j é o número de vezes que o neurônio i,j foi o vencedor.
        """
        self._check_input_len(data)
        a = zeros((self._weights.shape[0], self._weights.shape[1]))
        for x in data:
            a[self.winner(x)] += 1
        return a

    def _distance_from_weights(self, data):
        """Retorna uma matriz d onde d[i,j] é a distância euclidiana entre data[i] e o j-ésimo peso"""
        input_data = array(data)
        weights_flat = self._weights.reshape(-1, self._weights.shape[2])
        input_data_sq = power(input_data, 2).sum(axis=1, keepdims=True)
        weights_flat_sq = power(weights_flat, 2).sum(axis=1, keepdims=True)
        cross_term = dot(input_data, weights_flat.T)
        return sqrt(-2 * cross_term + input_data_sq + weights_flat_sq.T)

    def quantization_error(self, data):
        """Retorna o erro de quantização."""
        self._check_input_len(data)
        return norm(data-self.quantization(data), axis=1).mean()

    def distortion_measure(self, data):
        """Retorna a medida de distorção."""
        distortion = 0
        for d in data:
            distortion += multiply(self.neighborhood(self.winner(d),
                                                     self._sigma),
                                   norm(d - self.get_weights(), axis=2)).sum()
        return distortion

    def topographic_error(self, data):
        """Retorna o erro topográfico."""

        self._check_input_len(data)
        total_neurons = prod(self._activation_map.shape)
        if total_neurons == 1:
            warn('O erro topográfico não é definido para um mapa de 1x1.')
            return nan
        if self.topology == 'hexagonal':
            return self._topographic_error_hexagonal(data)
        else:
            return self._topographic_error_rectangular(data)

    def _topographic_error_hexagonal(self, data):
        """Retorna o erro topográfico para uma grade hexagonal"""
        b2mu_inds = argsort(self._distance_from_weights(data), axis=1)[:, :2]
        b2mu_coords = [[self._get_euclidean_coordinates_from_index(bmu[0]),
                        self._get_euclidean_coordinates_from_index(bmu[1])]
                       for bmu in b2mu_inds]
        b2mu_coords = array(b2mu_coords)
        b2mu_neighbors = [isclose(1, norm(bmu1 - bmu2))
                          for bmu1, bmu2 in b2mu_coords]
        te = 1 - mean(b2mu_neighbors)
        return te

    def _topographic_error_rectangular(self, data):
        """Retorna o erro topográfico para uma grade retangular"""
        t = 1.42
        # b2mu: best 2 matching units
        b2mu_inds = argsort(self._distance_from_weights(data), axis=1)[:, :2]
        b2my_xy = unravel_index(b2mu_inds, self._weights.shape[:2])
        b2mu_x, b2mu_y = b2my_xy[0], b2my_xy[1]
        dxdy = hstack([diff(b2mu_x), diff(b2mu_y)])
        distance = norm(dxdy, axis=1)
        return (distance > t).mean()

    def _get_euclidean_coordinates_from_index(self, index):
        """Retorna as coordenadas euclidianas de um neurônio usando seu índice como entrada"""
        if index < 0:
            return (-1, -1)
        y = self._weights.shape[1]
        coords = self.convert_map_to_euclidean((int(index/y), index % y))
        return coords

    def win_map(self, data, return_indices=False):
        """Retorna um dicionário com os padrões que foram mapeados para cada neurônio."""
        self._check_input_len(data)
        winmap = defaultdict(list)
        for i, x in enumerate(data):
            winmap[self.winner(x)].append(i if return_indices else x)
        return winmap

    def labels_map(self, data, labels):
        """Retorna um dicionário com a contagem de amostras de cada rótulo para cada neurônio."""
        self._check_input_len(data)
        if not len(data) == len(labels):
            raise ValueError('data e labels devem ter o mesmo comprimento.')
        winmap = defaultdict(list)
        for x, l in zip(data, labels):
            winmap[self.winner(x)].append(l)
        for position in winmap:
            winmap[position] = Counter(winmap[position])
        return winmap

if __name__ == "__main__":

    print("Iniciando o processamento do SOM...")

    data_path = r'C:\Users\Beatrix\Desktop\chirps\anomalias_norm_chirps.nc'
    ds = xr.open_dataset(data_path, engine='scipy')
    pr = ds['precip'].fillna(0.0)
    X = pr.values.reshape(pr.shape[0],-1)

    print("Dados carregados e formatados com sucesso.")
    print("Iniciando a criação do objeto MiniSom (6x6).")

    som = MiniSom(x=6,y=6,input_len=X.shape[1],sigma=1.0,learning_rate=0.5)

    print("Treinando o SOM...")
    som.train(X,100)

    print("Treinamento concluído. Gerando mapa de ativação.")
    activation = som.activation_response(X)  #

    plt.figure(figsize=(8, 8))
    plt.imshow(activation, cmap='viridis')
    plt.colorbar(label='Número de ativações')
    plt.title('Mapa de Ativação do SOM')
    plt.xlabel('Eixo X do SOM')
    plt.ylabel('Eixo Y do SOM')
    plt.show()
    
    print("Processamento finalizado.")
